{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdcb363b-93b6-4577-9eac-91989f9906d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import ensemble\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import shap\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 设置警告过滤器\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score, cross_val_predict\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abe54861-fc25-4304-a4cc-2c7c6960de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准化器已保存到 'fitted_scaler.pkl'\n",
      "新数据标准化完成，形状: (38, 16)\n",
      "加载字符映射: 9个字符, 最大长度: 18\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "新SMILES编码完成，形状: (38, 18, 9)\n",
      "(67, 32)\n",
      "数据已保存到 '留1燃料验证描述符.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "def set_random_seeds(seed=24):\n",
    "    random.seed(seed)          # Python内置随机种子\n",
    "    np.random.seed(seed)       # NumPy随机种子\n",
    "    tf.random.set_seed(seed)   # TensorFlow随机种子\n",
    "    # 对于GPU使用的额外设置\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "set_random_seeds(42)  # 设置为任意固定值\n",
    "# ====================================================\n",
    "# 1. 加载已有模型和标准化器\n",
    "# ====================================================\n",
    "\n",
    "# 加载已保存的标准化器（如果已保存）\n",
    "# 如果您之前已经保存了scaler，可以这样加载：\n",
    "# import joblib\n",
    "# scaler = joblib.load('fitted_scaler.pkl')\n",
    "\n",
    "# 如果没有保存，需要重新创建并使用训练数据拟合\n",
    "# 首先加载原始训练数据\n",
    "train_excel_file = 'processed_data-selected-feature-16 -plus.xlsx'\n",
    "train_df = pd.read_excel(train_excel_file)\n",
    "\n",
    "# 提取训练数据的描述符（使用与训练时相同的列）\n",
    "train_descriptors = train_df.iloc[:, 4:30]\n",
    "\n",
    "# 创建并拟合标准化器（使用训练数据）\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_descriptors)  # 只使用训练数据拟合\n",
    "\n",
    "# 保存标准化器以备将来使用\n",
    "import joblib\n",
    "joblib.dump(scaler, 'fitted_scaler.pkl')\n",
    "print(\"标准化器已保存到 'fitted_scaler.pkl'\")\n",
    "\n",
    "# ====================================================\n",
    "# 2. 加载和处理新数据\n",
    "# ====================================================\n",
    "\n",
    "# 加载新数据\n",
    "new_excel_file = 'dataset-留1燃料验证法.xlsx'\n",
    "new_df = pd.read_excel(new_excel_file)\n",
    "\n",
    "# 提取新数据的SMILES和描述符\n",
    "new_smiles_column = new_df['SMILES'].astype(str).tolist()\n",
    "\n",
    "# 确保新数据的描述符列与训练数据相同\n",
    "new_descriptors = new_df.iloc[:, 4:20]  # 假设列索引相同\n",
    "\n",
    "# 使用训练数据拟合的标准化器对新数据进行标准化\n",
    "new_descriptors_scaled = scaler.transform(new_descriptors)\n",
    "print(f\"新数据标准化完成，形状: {new_descriptors_scaled.shape}\")\n",
    "\n",
    "# ====================================================\n",
    "# 3. 对新数据的SMILES进行One-Hot编码\n",
    "# ====================================================\n",
    "\n",
    "# 加载已有的字符到索引映射\n",
    "# 如果您之前保存了char_to_index和max_len\n",
    "# 假设您已经从训练过程中获取了这些参数\n",
    "# 这里需要从原始训练代码中获取或保存这些参数\n",
    "\n",
    "# 方法1：从保存的文件中加载\n",
    "try:\n",
    "    import pickle\n",
    "    with open('char_to_index.pkl', 'rb') as f:\n",
    "        char_to_index = pickle.load(f)\n",
    "    with open('max_len.pkl', 'rb') as f:\n",
    "        max_len = pickle.load(f)\n",
    "    print(f\"加载字符映射: {len(char_to_index)}个字符, 最大长度: {max_len}\")\n",
    "except FileNotFoundError:\n",
    "    # 方法2：重新计算（需要原始训练数据）\n",
    "    print(\"未找到保存的映射文件，重新计算...\")\n",
    "    all_smiles = train_df['smiles'].astype(str).tolist() + new_smiles_column\n",
    "    all_characters = set(''.join(all_smiles))\n",
    "    char_to_index = {char: idx for idx, char in enumerate(sorted(all_characters))}\n",
    "    max_len = max(len(smiles) for smiles in all_smiles)\n",
    "    \n",
    "    # 保存映射\n",
    "    with open('char_to_index.pkl', 'wb') as f:\n",
    "        pickle.dump(char_to_index, f)\n",
    "    with open('max_len.pkl', 'wb') as f:\n",
    "        pickle.dump(max_len, f)\n",
    "\n",
    "def encode_smiles(smiles, char_to_index, max_len):\n",
    "    \"\"\"对单个SMILES进行One-Hot编码\"\"\"\n",
    "    one_hot_matrix = np.zeros((max_len, len(char_to_index)), dtype=int)\n",
    "    for i, char in enumerate(smiles):\n",
    "        if char in char_to_index:\n",
    "            one_hot_matrix[i, char_to_index[char]] = 1\n",
    "        else:\n",
    "            print(f\"警告: 字符 '{char}' 不在训练词汇表中\")\n",
    "    return one_hot_matrix\n",
    "\n",
    "# 对新数据的SMILES进行编码\n",
    "new_smiles_encoded = np.array([\n",
    "    encode_smiles(smile, char_to_index, max_len) \n",
    "    for smile in new_smiles_column\n",
    "])\n",
    "print(new_smiles_encoded)\n",
    "print(f\"新SMILES编码完成，形状: {new_smiles_encoded.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# 5. 构建确定性模型（关键修改）\n",
    "# ====================================================\n",
    "def create_deterministic_model(max_len, num_chars, descriptor_dim):\n",
    "    smiles_input = Input(shape=(max_len, num_chars), name='smiles_input')\n",
    "    descriptors_input = Input(shape=(descriptor_dim,), name='descriptors_input')\n",
    "    \n",
    "    # 使用固定初始化器\n",
    "    x = Flatten()(smiles_input)\n",
    "    x = Dense(16, \n",
    "              activation='relu',\n",
    "              kernel_initializer=tf.keras.initializers.GlorotUniform(seed=42))(x)\n",
    "    \n",
    "    merged = Concatenate()([x, descriptors_input])\n",
    "    model = Model(inputs=[smiles_input, descriptors_input], outputs=merged)\n",
    "    \n",
    "    # 固定编译参数\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_deterministic_model(\n",
    "    max_len=max_len,\n",
    "    num_chars=len(char_to_index),\n",
    "    descriptor_dim=X_train_descriptors.shape[1]\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# 6. 获取确定性特征（关键修改）\n",
    "# ====================================================\n",
    "# 第一次运行会初始化权重，之后固定\n",
    "X_new_smiles_train = model.predict([new_smiles_encoded, new_descriptors_scaled], verbose=0)\n",
    "print(X_train.shape)\n",
    "features_df = pd.DataFrame(X_new_smiles_train)\n",
    "output_file = '留1燃料验证描述符.xlsx'\n",
    "features_df.to_excel(output_file, index=False)\n",
    "print(f\"数据已保存到 '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa961ce0-8b6c-4ea0-be4c-58c24773b866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "集成模型加载成功\n",
      "(38, 32)\n",
      "预测结果: [29.6468577  36.36429069 41.33291258 42.22561459 40.49532777 34.77717198\n",
      " 28.46546979 21.11639527 15.04792168 12.96426063 34.96443729 42.5610286\n",
      " 47.17113026 48.6727407  46.29867414 40.23310849 33.1211635  23.89275049\n",
      " 16.74220693 12.93226094 29.2818993  36.06631168 41.08604918 42.01229055\n",
      " 40.43820678 35.18838836 28.85470508 21.4667801  15.3445811  13.19488108\n",
      " 37.50864    46.26168865 51.00131246 52.72007396 50.32091041 44.37432349\n",
      " 37.48240977 27.09445431]\n",
      "逐行打印预测结果:\n",
      " 29.646857699838485\n",
      " 36.364290691471574\n",
      " 41.332912583041214\n",
      " 42.22561458784122\n",
      " 40.495327765203164\n",
      " 34.77717198398362\n",
      " 28.46546979303836\n",
      " 21.11639526567103\n",
      " 15.047921684182995\n",
      " 12.964260625465172\n",
      " 34.96443729130808\n",
      " 42.5610286040313\n",
      " 47.17113026081514\n",
      " 48.67274070270621\n",
      " 46.29867413821669\n",
      " 40.23310849123063\n",
      " 33.12116350397023\n",
      " 23.892750493547922\n",
      " 16.7422069292033\n",
      " 12.93226094390981\n",
      " 29.281899295610557\n",
      " 36.06631168405235\n",
      " 41.086049183440416\n",
      " 42.01229054627568\n",
      " 40.43820678457948\n",
      " 35.18838836054741\n",
      " 28.854705076705656\n",
      " 21.466780098807853\n",
      " 15.344581102582275\n",
      " 13.19488108009827\n",
      " 37.50864000365891\n",
      " 46.26168865120564\n",
      " 51.00131246358251\n",
      " 52.720073961040534\n",
      " 50.32091041420595\n",
      " 44.37432349344253\n",
      " 37.48240977394754\n",
      " 27.094454307733756\n"
     ]
    }
   ],
   "source": [
    "# 加载保存的集成模型\n",
    "ensemble_model = joblib.load('voting_regressor_model-LFS-3754.pkl')\n",
    "print(\"集成模型加载成功\")\n",
    "\n",
    "# 标准化描述符数据\n",
    "print(X_new_smiles_train.shape)\n",
    "# 使用训练好的模型进行预测\n",
    "y_pred_new = ensemble_model.predict(X_new_smiles_train)\n",
    "\n",
    "# 输出预测结果\n",
    "print(f'预测结果: {y_pred_new}')\n",
    "# 将预测结果添加到 DataFrame 中，假设我们添加到 'Predicted_LFS' 列\n",
    "print(\"逐行打印预测结果:\")\n",
    "for idx, prediction in enumerate(y_pred_new):\n",
    "    print(f' {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "786195a7-2153-4684-977e-7e88b3cd824c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 32)\n",
      "预测结果: [34.9496563  42.58245144 40.02592303 42.55375274 40.20140694 59.04404589\n",
      " 37.62697003 46.01739176 66.55143158 38.30855874 39.35595203 34.10507539\n",
      " 35.99421407 43.5871489  53.70993342 40.88979897 37.73473497 43.62270909\n",
      " 47.84616124 47.26714609 46.61179029 51.28056928 47.76439238 61.71573117\n",
      " 41.24592732 43.55945077 51.82650964]\n",
      "逐行打印预测结果:\n",
      " 34.94965630061602\n",
      " 42.582451437526984\n",
      " 40.02592302524653\n",
      " 42.55375274120963\n",
      " 40.20140693956762\n",
      " 59.04404588711128\n",
      " 37.626970034953395\n",
      " 46.01739175534136\n",
      " 66.55143158205738\n",
      " 38.30855874215168\n",
      " 39.355952032341285\n",
      " 34.10507539199374\n",
      " 35.99421406613353\n",
      " 43.58714889802576\n",
      " 53.70993341751661\n",
      " 40.88979896605902\n",
      " 37.73473496920412\n",
      " 43.622709086043514\n",
      " 47.8461612355205\n",
      " 47.26714609156808\n",
      " 46.61179029025832\n",
      " 51.280569284000485\n",
      " 47.764392382716295\n",
      " 61.71573116595642\n",
      " 41.245927322387416\n",
      " 43.559450771247164\n",
      " 51.826509638713794\n"
     ]
    }
   ],
   "source": [
    "file_path = '预测数据集.xlsx'  # 请替换为新数据的文件路径\n",
    "df_new = pd.read_excel(file_path)\n",
    "# print(df_new)\n",
    "X_new =df_new.iloc[:,9:41] \n",
    "# 标准化描述符数据\n",
    "print(X_new.shape)\n",
    "# 使用训练好的模型进行预测\n",
    "y_pred_new = ensemble_model.predict(X_new)\n",
    "\n",
    "# 输出预测结果\n",
    "print(f'预测结果: {y_pred_new}')\n",
    "# 将预测结果添加到 DataFrame 中，假设我们添加到 'Predicted_LFS' 列\n",
    "print(\"逐行打印预测结果:\")\n",
    "for idx, prediction in enumerate(y_pred_new):\n",
    "    print(f' {prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
